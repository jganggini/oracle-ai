{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d177d97-18fe-47d3-897f-fa16a5a96924",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Working RAG with Generative AI ü§ñ\n",
    "\n",
    "Data Science utiliza un Modelo LLMs [Cohere Command R+] para analizar los documentos indexados en OpenSearch y realizar acciones basadas en el contenido de los documentos. El modelo es personalizado en Data Science y desplegado para poder ser consumido por las aplicaciones.\n",
    "\n",
    "[![Notebook Examples](https://img.shields.io/badge/docs-notebook--examples-blue)](https://github.com/oracle-samples/oci-data-science-ai-samples/tree/master/notebook_examples)\n",
    "[![Conda Environments](https://img.shields.io/badge/docs-conda--environments-blue)](https://docs.oracle.com/en-us/iaas/data-science/using/conda_understand_environments.htm)\n",
    "[![Source Code](https://img.shields.io/badge/source-accelerated--datascience-blue)](https://github.com/oracle/accelerated-data-science)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005bdbb7-e228-406e-89b9-1e40b6f545e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### [Step-01] Libraries\n",
    "\n",
    "<details>\n",
    "<summary><font size=\"2\">Install Pre-Requirements</font></summary>\n",
    "<font size=\"1\">\n",
    "\n",
    "```Install Libraries\n",
    "(base) bash-4.2$ odsc conda create -f environment.yaml -n langchain_env -v 1.5\n",
    "```\n",
    "    \n",
    "</font>    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2effae89-8a69-438d-81cb-69fa549d0164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import oci\n",
    "import ads\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.oci_generative_ai.OCIGenAIEmbeddings.html\n",
    "from langchain_community.embeddings.oci_generative_ai import OCIAuthType\n",
    "from langchain_community.embeddings.oci_generative_ai import OCIGenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b6052-5567-4f6c-bbea-ac29383f057b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### [Step-02] Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df03b637-7298-4a64-b493-2c13adb7db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oci: Generative AI\n",
    "compartment_id   = os.environ['NB_SESSION_COMPARTMENT_OCID']\n",
    "service_endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "genai_embeddings = 'cohere.embed-multilingual-v3.0'\n",
    "genai_inference  = \"ocid1.generativeaimodel.oc1.us-chicago-1.***\"\n",
    "auth_type        = \"RESOURCE_PRINCIPAL\"\n",
    "\n",
    "# oci: OpenSearch\n",
    "apiEndpoint = \"http://***.***.***.***:9200\"\n",
    "username    = \"opensearch\"\n",
    "password    = \"************\"\n",
    "searchIndex = \"oci_documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fdb5e4-fe24-4f9e-8db5-8107ef5ed67b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### [Step-01] Opensearch\n",
    "Function to retrieve documents from OpenSearch based on the provided obj_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e4a720-d056-4e97-88f7-e49614aab28b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_documents_opensearch(obj_url):\n",
    "    content = \"\"    \n",
    "    # Construct the search URL for the OpenSearch endpoint\n",
    "    queryurl = f\"{apiEndpoint}/{searchIndex}/_search\"\n",
    "    # Set the authentication credentials for the OpenSearch request\n",
    "    auth = (username, password)\n",
    "    # Set the headers for the OpenSearch request\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    # Construct the OpenSearch query\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    { \"term\": { \"obj_url.keyword\": obj_url } }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"obj_content\"]  # Fetch only the obj_content field\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send a GET request to the OpenSearch endpoint with the constructed query\n",
    "        resp = requests.get(queryurl, auth=auth, headers=headers, data=json.dumps(query))\n",
    "        # Raise an exception if the request returned an unsuccessful status code\n",
    "        resp.raise_for_status()\n",
    "        # Parse the JSON response from the request\n",
    "        response = resp.json()\n",
    "        \n",
    "        # Iterate over the search results in the response\n",
    "        for hit in response['hits']['hits']:\n",
    "            content += hit['_source']['obj_content'] + \"\\n\\n\"\n",
    "          \n",
    "        return content        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[get_documents_opensearch] {str(e)}.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5667b-0c59-4b57-95bc-bd9aa30631a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### [Step-02] Embedding\n",
    "Function to retrieve embeddings from OCI Generative AI service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6802f13d-85fd-42e0-a818-e12245c4270a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    try:\n",
    "        # Initialize the OCIGenAIEmbeddings object with the specified parameters\n",
    "        embeddings = OCIGenAIEmbeddings(\n",
    "            model_id         = genai_embeddings, # Specify the model ID for embeddings\n",
    "            service_endpoint = service_endpoint,\n",
    "            compartment_id   = compartment_id,\n",
    "            auth_type        = auth_type # Authentication type\n",
    "        )\n",
    "        \n",
    "        return embeddings\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[get_embeddings_model] {str(e)}.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb873694-1013-425f-9729-55793967eccb",
   "metadata": {
    "tags": []
   },
   "source": [
    " ##### [Step-03] Retriever\n",
    " Function to generate content embeddings and create a retriever for searching the content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9d964d-47e7-4274-91f2-0abacf42afc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_content(text, embeddings):    \n",
    "    try:\n",
    "        # Generate embeddings for the content\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size         = 1000,  # Size of each text chunk\n",
    "            chunk_overlap      = 50,    # Overlap between chunks\n",
    "            length_function    = len,   # Function to measure the length of text\n",
    "            is_separator_regex = False  # Whether the separator is a regex\n",
    "        )\n",
    "        \n",
    "        # Split the text into chunks\n",
    "        chunks = text_splitter.split_text(text)\n",
    "\n",
    "        # Create the FAISS vector store with the embeddings\n",
    "        VectorStore = FAISS.from_texts(chunks, embeddings)\n",
    "\n",
    "        # Convert the vector store to a retriever\n",
    "        retriever = VectorStore.as_retriever(search_kwargs={\"k\": 1000}) # Number of top results to retrieve\n",
    "        \n",
    "        return retriever\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[get_content] {str(e)}.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401758ac-aa04-4fdc-b2a7-3084ae214b1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### [Step-04] LLM \n",
    "\n",
    "<details>\n",
    "<summary><font size=\"2\">ChatOCIGenAI</font></summary>\n",
    "<font size=\"1\">\n",
    "        \n",
    "```text\n",
    "> model_id: Specify the model ID for the chat model \"cohere.command-r-plus\".\n",
    "> service_endpoint: The service endpoint for OCI Generative AI\n",
    "> compartment_id: OCI compartment ID\n",
    "> provider: The provider of the model\n",
    "> is_stream: Whether to use streaming for responses\n",
    "> auth_type: Authentication type\n",
    "> max_tokens: Maximum number of tokens in the response\n",
    "> temperature: Sampling temperature\n",
    "> top_p: Nucleus sampling parameter\n",
    "> top_k: Top-K sampling parameter\n",
    "> frequency_penalty: Frequency penalty for token repetition\n",
    "```\n",
    "        \n",
    "</font>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01e3b71-e61b-4b55-98fb-c227c847f1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aqu√≠ est√° un resumen del contexto proporcionado:\\n\\nEl documento parece ser una lista de art√≠culos y repuestos con sus respectivas descripciones, cantidades y valores ofrecidos. Hay un total de 40 art√≠culos en la lista, que van desde retenes de aceite y pernos hasta filtros de aire y aceite, fusibles y varios tipos de cintas aisladoras. Cada art√≠culo tiene un n√∫mero de orden, una descripci√≥n detallada, la categor√≠a a la que pertenece y el valor ofrecido. El documento tambi√©n incluye informaci√≥n sobre el subtotal, el IVA y el total de la transacci√≥n, aunque todos estos valores son 0 en este caso particular.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the object URL for the document in OCI Object Storage\n",
    "obj_url   = \"https://objectstorage.us-chicago-1.oraclecloud.com/n/idi1o0a010nx/b/DLK1LAGDEV/o/example.xlsx\"\n",
    "\n",
    "# Retrieve the document text from OpenSearch based on the object URL\n",
    "text = get_documents_opensearch(obj_url)\n",
    "\n",
    "# Get the embeddings using the specified model and configuration\n",
    "embeddings = get_embeddings()\n",
    "\n",
    "# Generate content embeddings and create a retriever for searching the content\n",
    "retriever   = get_content(text, embeddings)\n",
    "\n",
    "# Initialize the ChatOCIGenAI object with the specified parameters\n",
    "chat = ChatOCIGenAI(\n",
    "    model_id         = \"cohere.command-r-plus\",\n",
    "    service_endpoint = service_endpoint,\n",
    "    compartment_id   = compartment_id,\n",
    "    provider         = \"cohere\",\n",
    "    is_stream        = True,\n",
    "    auth_type        = auth_type,\n",
    "    model_kwargs     = {\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 20,\n",
    "        \"frequency_penalty\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define a prompt template for the chat model\n",
    "prompt_template = PromptTemplate.from_template(\"{query}, bas√°ndose √∫nicamente en el siguiente contexto: {context}\")\n",
    "\n",
    "# Create a processing chain with the query, retriever, prompt template, chat model, and output parser\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough(), \"context\": retriever}  # Pass the query and retriever context\n",
    "    | prompt_template                                       # Apply the prompt template\n",
    "    | chat                                                  # Invoke the chat model\n",
    "    | StrOutputParser()                                     # Parse the output as a string\n",
    ")\n",
    "\n",
    "# Invoke the chain with the query (max: 250 character)\n",
    "chain.invoke(\"Generar un resumen\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain_env_v1_5]",
   "language": "python",
   "name": "conda-env-langchain_env_v1_5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
