{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1647d5d-9e6b-427f-aa00-3dde240f54ad",
   "metadata": {},
   "source": [
    "# 游 RAG with Generative AI (Deployment)\n",
    "\n",
    "The ads.model.generic_model.GenericModel class in ADS provides an efficient way to serialize almost any model class. This section demonstrates how to use the GenericModel class to prepare model artifacts, verify models, save models to the model catalog, deploy models, and perform predictions on model deployment endpoints.\n",
    "\n",
    "[![Notebook Examples](https://img.shields.io/badge/docs-notebook--examples-blue)](https://accelerated-data-science.readthedocs.io/en/latest/user_guide/model_registration/frameworks/genericmodel.html)\n",
    "[![Conda Environments](https://img.shields.io/badge/docs-conda--environments-blue)](https://docs.oracle.com/en-us/iaas/data-science/using/conda_understand_environments.htm)\n",
    "[![Source Code](https://img.shields.io/badge/source-accelerated--datascience-blue)](https://github.com/oracle/accelerated-data-science)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ea20c-a295-4aef-9dbd-91a815fd04ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### [Step-01] CustomModel\n",
    "\n",
    "<details>\n",
    "<summary><font size=\"2\">Install Pre-Requirements</font></summary>\n",
    "<font size=\"1\">\n",
    "\n",
    "```Install Libraries\n",
    "(base) bash-4.2$ odsc conda create -f environment.yaml -n langchain_env -v 1.5\n",
    "```\n",
    "    \n",
    "</font>    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aa65ef-584a-48e8-ba06-89927ead0e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import oci\n",
    "import ads\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import tempfile\n",
    "from ads.model.generic_model import GenericModel\n",
    "from langchain_community.embeddings.oci_generative_ai import OCIGenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class CustomModel:\n",
    "    def __init__(self):\n",
    "        # oci: Generative AI\n",
    "        self.compartment_id   = os.environ['NB_SESSION_COMPARTMENT_OCID']\n",
    "        self.service_endpoint = \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\"\n",
    "        self.genai_embeddings = 'cohere.embed-multilingual-v3.0'\n",
    "        self.genai_inference  = \"ocid1.generativeaimodel.oc1.us-chicago-1.***\"\n",
    "        self.auth_type        = \"RESOURCE_PRINCIPAL\"\n",
    "        # oci: OpenSearch\n",
    "        self.apiEndpoint      = \"http://***.***.***.***:9200\"\n",
    "        self.username         = \"opensearch\"\n",
    "        self.password         = \"*************\"\n",
    "        self.searchIndex      = \"oci_documents\"\n",
    "    \n",
    "    # Function to retrieve documents from OpenSearch based on the provided obj_url\n",
    "    def get_documents_opensearch(self, obj_url):\n",
    "        content = \"\"\n",
    "        # Construct the search URL for the OpenSearch endpoint\n",
    "        queryurl = f\"{self.apiEndpoint}/{self.searchIndex}/_search\"\n",
    "        # Set the authentication credentials for the OpenSearch request\n",
    "        auth = (self.username, self.password)\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        # Construct the OpenSearch query\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"must\": [\n",
    "                        { \"term\": { \"obj_url.keyword\": obj_url } }\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"_source\": [\"obj_content\"]  # Fetch only the obj_content field\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Send a GET request to the OpenSearch endpoint with the constructed query\n",
    "            resp = requests.get(queryurl, auth=auth, headers=headers, data=json.dumps(query))\n",
    "            # Raise an exception if the request returned an unsuccessful status code\n",
    "            resp.raise_for_status()\n",
    "            # Parse the JSON response from the request\n",
    "            response = resp.json()\n",
    "\n",
    "            # Iterate over the search results in the response\n",
    "            for hit in response['hits']['hits']:\n",
    "                content += hit['_source']['obj_content'] + \"\\n\\n\"\n",
    "\n",
    "            return content\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[get_documents_opensearch] {str(e)}.\")\n",
    "            raise\n",
    "            \n",
    "    # Function to retrieve embeddings from OCI Generative AI service\n",
    "    def get_embeddings(self):\n",
    "        try:\n",
    "            # Initialize the OCIGenAIEmbeddings object with the specified parameters\n",
    "            embeddings = OCIGenAIEmbeddings(\n",
    "                model_id         = self.genai_embeddings, # Specify the model ID for embeddings\n",
    "                service_endpoint = self.service_endpoint,\n",
    "                compartment_id   = self.compartment_id,\n",
    "                auth_type        = self.auth_type # Authentication type\n",
    "            )\n",
    "\n",
    "            return embeddings\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[get_embeddings_model] {str(e)}.\")\n",
    "            raise\n",
    "            \n",
    "    #  Function to generate content embeddings and create a retriever for searching the content\n",
    "    def get_content(self, text, embeddings):    \n",
    "        try:\n",
    "            # Generar los embeddings del contenido\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size         = 1000,  # Size of each text chunk\n",
    "                chunk_overlap      = 50,    # Overlap between chunks\n",
    "                length_function    = len,   # Function to measure the length of text\n",
    "                is_separator_regex = False  # Whether the separator is a regex\n",
    "            )\n",
    "\n",
    "            # Split the text into chunks\n",
    "            chunks = text_splitter.split_text(text)\n",
    "\n",
    "            # Create the FAISS vector store with the embeddings\n",
    "            VectorStore = FAISS.from_texts(chunks, embeddings)\n",
    "\n",
    "            # Convert the vector store to a retriever\n",
    "            retriever = VectorStore.as_retriever(search_kwargs={\"k\": 1000}) # Number of top results to retrieve\n",
    "\n",
    "            return retriever\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[get_content] {str(e)}.\")\n",
    "            raise\n",
    "    \n",
    "    def predict(self, data):\n",
    "        # Define question for LLM\n",
    "        question = data[\"input\"][0]\n",
    "        \n",
    "        # Define the object URL for the document in OCI Object Storage\n",
    "        obj_url  = data[\"input\"][1]\n",
    "        \n",
    "        # Retrieve the document text from OpenSearch based on the object URL\n",
    "        text = self.get_documents_opensearch(obj_url)\n",
    "        \n",
    "        # Get the embeddings using the specified model and configuration \n",
    "        embeddings = self.get_embeddings()\n",
    "\n",
    "        # Generate content embeddings and create a retriever for searching the content\n",
    "        retriever   = self.get_content(text, embeddings)\n",
    "\n",
    "        chat = ChatOCIGenAI(\n",
    "            model_id         = \"cohere.command-r-plus\",\n",
    "            service_endpoint = self.service_endpoint,\n",
    "            compartment_id   = self.compartment_id,\n",
    "            provider         = \"cohere\",\n",
    "            is_stream        = True,\n",
    "            auth_type        = self.auth_type,\n",
    "            model_kwargs     = {\n",
    "                \"max_tokens\": 512,\n",
    "                \"temperature\": 0.6,\n",
    "                \"top_p\": 0.9,\n",
    "                \"top_k\": 20,\n",
    "                \"frequency_penalty\": 1\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Define a prompt template for the chat model\n",
    "        prompt_template = ChatPromptTemplate.from_template(\"{query}, bas치ndose 칰nicamente en el siguiente contexto: {context}\")\n",
    "        \n",
    "        # Create a processing chain with the query, retriever, prompt template, chat model, and output parser\n",
    "        chain = (\n",
    "            {\"query\": RunnablePassthrough(), \"context\": retriever}  # Pass the query and retriever context\n",
    "            | prompt_template                                       # Apply the prompt template\n",
    "            | chat                                                  # Invoke the chat model\n",
    "            | StrOutputParser()                                     # Parse the output as a string\n",
    "        )\n",
    "        \n",
    "        # Invoke the chain with the query (max: 250 character)\n",
    "        return chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5671b4-465d-475a-ba26-f2b8cfa024bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Aqu칤 est치 un resumen del contexto proporcionado:\\n\\nEl documento parece ser una lista de art칤culos y repuestos con sus descripciones, cantidades y precios ofrecidos. Hay un total de 40 art칤culos en la lista, que van desde retenes de aceite y pernos hasta filtros de aire y aceite, descalcificadores, fusibles y varios tipos de cables. Los precios ofrecidos var칤an, con algunos art칤culos cotizados en cantidades m치s grandes. El subtotal, IVA y total est치n listados como $0, lo que sugiere que este es un listado de precios sin un total calculado. La lista parece estar dirigida a repuestos y accesorios para autom칩viles, con referencias a marcas espec칤ficas como Hyundai, Kia, Toyota y Yohama.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the custom model object\n",
    "model = CustomModel()\n",
    "\n",
    "# Use the model to make a prediction with the given input\n",
    "model.predict({\"input\": \n",
    "     [\"Generar un resumen\",  # The first input is the task or query\n",
    "      \"https://objectstorage.us-chicago-1.oraclecloud.com/n/idi1o0a010nx/b/DLK1LAGDEV/o/example.xlsx\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acafa50-f6c0-4abe-b6a7-38bf92fbc6c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### [Step-02] Prepare Model\n",
    "\n",
    "<details>\n",
    "<summary><font size=\"2\">Pre-Requirements</font></summary>\n",
    "<font size=\"1\">\n",
    "\n",
    "```Terminal\n",
    "!(base) bash-4.2$ odsc conda init -b <bucket-name> -n <namespace> -a <api_key or resource_principal>\n",
    "!(base) bash-4.2$ odsc conda init -b <bucket-name> -n <namespace> -a resource_principal\n",
    "!(base) bash-4.2$ odsc conda publish -s <slug>\n",
    "!(base) bash-4.2$ odsc conda publish -s langchain_env_v1_5\n",
    "<pack> =\"oci://<bucket-name>@<namespace>/conda_environments/cpu/langchain_env/1.5/langchain_env_v1_5\"\n",
    "```    \n",
    "</font>    \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><font size=\"2\">Optional</font></summary>\n",
    "<font size=\"1\">\n",
    "\n",
    "```Terminal\n",
    "!(base) bash-4.2$ odsc conda delete -s <slug>\n",
    "!(base) bash-4.2$ odsc conda delete -s langchain_env_v1_5\n",
    "```\n",
    "    \n",
    "</font>    \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><font size=\"2\">generic_model.prepare</font></summary>\n",
    "<font size=\"1\">\n",
    "\n",
    "```Terminal\n",
    "generic_model.prepare(\n",
    "    inference_conda_env = <pack>,\n",
    "    inference_python_version = \"3.8\",\n",
    "    model_file_name = \"langchain.pkl\",\n",
    "    force_overwrite = True\n",
    ")\n",
    "```    \n",
    "</font>    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e42b11f-c670-406b-98af-63acb74cd221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                            ?, ?it/s]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "algorithm: null\n",
       "artifact_dir:\n",
       "  /tmp/tmp1761pyeq:\n",
       "  - - .model-ignore\n",
       "    - langchain.pkl\n",
       "    - runtime.yaml\n",
       "    - score.py\n",
       "framework: null\n",
       "model_deployment_id: null\n",
       "model_id: null"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import oci\n",
    "import ads\n",
    "import tempfile\n",
    "from ads.model.generic_model import GenericModel\n",
    "\n",
    "ads.set_auth(\"resource_principal\")\n",
    "\n",
    "generic_model = GenericModel(\n",
    "    estimator = model,\n",
    "    artifact_dir=tempfile.mkdtemp(),\n",
    "    model_save_serializer=\"cloudpickle\",\n",
    "    model_input_serializer=\"json\"\n",
    ")\n",
    "generic_model.summary_status()\n",
    "\n",
    "generic_model.prepare(\n",
    "    inference_conda_env=\"oci://DLK4CUR@idi1o0a010nx/conda_environments/cpu/langchain_env/1.5/langchain_env_v1_5\",\n",
    "    inference_python_version=\"3.8\",\n",
    "    model_file_name=\"langchain.pkl\",\n",
    "    force_overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed355d3a-4217-4c97-955c-fadfa3115f9e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### [Step-03] Check Model\n",
    "The verify method invokes the ``predict`` function defined inside ``score.py`` in the artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9c8fd9-c218-4216-a80e-66e16b77f2d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading langchain.pkl from model directory /tmp/tmp1761pyeq ...\n",
      "Model is successfully loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prediction': 'Aqu칤 est치 un resumen del contexto proporcionado:\\n\\nEl documento parece ser una lista de art칤culos y repuestos con sus descripciones, cantidades y precios ofrecidos. Hay un total de 40 art칤culos en la lista, que van desde retenes de aceite y pernos hasta filtros de aire y aceite, fusibles y varios tipos de cintas aisladoras. Los precios ofrecidos var칤an, pero en este caso espec칤fico, el subtotal, el IVA y el total son todos $0, lo que sugiere que esta es solo una lista de art칤culos sin precios definitivos o que faltan detalles de precios. La lista incluye una variedad de piezas para autom칩viles y equipos el칠ctricos, con cantidades que van desde unos pocos hasta varios cientos.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The verify method invokes the ``predict`` function defined inside ``score.py`` in the artifact_dir\n",
    "generic_model.verify({\"input\": \n",
    "     [\"Generar un resumen\",  # The first input is the task or query\n",
    "      \"https://objectstorage.us-chicago-1.oraclecloud.com/n/idi1o0a010nx/b/DLK1LAGDEV/o/example.xlsx\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7c2c1-0508-46c7-b954-3b6d0de3c036",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### [Step-04] Save Model\n",
    "Save the generic model with the specified display name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cdcf685-1477-4750-92ed-682d93961b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading langchain.pkl from model directory /tmp/tmp1761pyeq ...\n",
      "Model is successfully loaded.\n",
      "['.model-ignore', 'langchain.pkl', 'runtime.yaml', 'score.py']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10be2a562d6f43709298f0ef9f4d8ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ocid1.datasciencemodel.oc1.iad.amaaaaaafioir7ia233xjyp6t3iifz3y22s5p2zadhdj2qqdghcbfmii6xzq'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generic_model.save(display_name=\"RAG Generative AI v1.8.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61da61c-0e98-4c05-89be-ce54030c19ed",
   "metadata": {},
   "source": [
    "##### [Step-05] Deployment\n",
    "Deploy the generic model with the specified parameters\n",
    "\n",
    "<details>\n",
    "<summary><font size=\"2\">Oracle Cloud</font></summary>\n",
    "<font size=\"1\">\n",
    "\n",
    "```Steps\n",
    "> Data Science\n",
    "> Proyects (Select Project)\n",
    "> Model Deployments\n",
    "> Create Model Deployment\n",
    "```\n",
    "</font>    \n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><font size=\"2\">Create Model Deployment</font></summary>\n",
    "<font size=\"1\">\n",
    "\n",
    "```Generate Pack\n",
    "> Name: Deploy RAG Generative AI v1.8.1\n",
    "> Models: RAG Generative AI v1.8.1\n",
    "> Shape: VM.Standard.E4.Flex\n",
    "> Networking resources: Custom networking (VCN/Public Subnet)\n",
    "```    \n",
    "</font>    \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><font size=\"2\"> Deploy RAG Generative AI v.1.8.2 > Invoking Your Model</font></summary>\n",
    "<img src=\"img/deployment.png\" alt=\"Invoking your Model\" width=\"90%\">\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf8ca0-0da5-4c44-9bfa-c3746c57ef89",
   "metadata": {},
   "source": [
    "##### [Step-06] Test\n",
    "The OCI SDK must be installed for this example to function properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99bec4eb-9891-468e-b140-3019b889234b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': 'Aqu칤 est치 un resumen del contexto proporcionado:\\n\\nEl documento parece ser una lista de art칤culos y repuestos con sus descripciones, cantidades y precios ofrecidos. Hay 40 art칤culos en total, que van desde retenes de aceite y pernos hasta filtros de aire y aceite, fusibles y varios tipos de cables. Los precios ofrecidos var칤an, pero el subtotal, el IVA y el total en la parte inferior del documento est치n establecidos en $0, lo que sugiere que este es solo un listado de art칤culos sin precios definitivos.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import oci\n",
    "import ads\n",
    "import requests\n",
    "\n",
    "# Supported values: resource_principal, api_key\n",
    "ads.set_auth(\"resource_principal\") \n",
    "signer = oci.auth.signers.get_resource_principals_signer()\n",
    "\n",
    "endpoint = \"https://modeldeployment.us-ashburn-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.iad.amaaaaaafioir7iat3zjurhoqro7e2kx76stea2iv2bupali55l2l5inwscq/predict\"\n",
    "body = {\n",
    "    \"input\": [\n",
    "        \"Generar un resumen\",\n",
    "        \"https://objectstorage.us-chicago-1.oraclecloud.com/n/idi1o0a010nx/b/DLK1LAGDEV/o/example.xlsx\"\n",
    "    ]\n",
    "}\n",
    "headers = {} # header goes here\n",
    "\n",
    "requests.post(endpoint, json=body, auth=signer, headers=headers).json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:langchain_env_v1_5]",
   "language": "python",
   "name": "conda-env-langchain_env_v1_5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
