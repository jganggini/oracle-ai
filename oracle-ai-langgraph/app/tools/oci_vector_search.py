import os


from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI

from dotenv import load_dotenv
import services.database as database

# Initialize the service
db_doc_service = database.DocService()

# Load environment variables from the .env file
load_dotenv()

# Initialize the OCI Generative AI Chat Model
llm = ChatOCIGenAI(
    model_id         = os.getenv("CON_GEN_AI_CHAT_MODEL_ID"),
    service_endpoint = os.getenv("CON_GEN_AI_SERVICE_ENDPOINT"),
    compartment_id   = os.getenv("CON_COMPARTMENT_ID"),
    provider         = os.getenv("CON_GEN_AI_CHAT_MODEL_PROVIDER"),
    is_stream        = True,
    auth_type        = os.getenv("CON_GEN_AI_AUTH_TYPE"),
    model_kwargs     = {
        "temperature" : 0,
    }
)

def oci_vector_search(input: str):
    """
    Performs a vector search using an Oracle database and OCI embeddings.
    
    Args:
        input (str): The input query to search for relevant documents.
        
    Returns:
        str: The answer generated by the retrieval-augmented generation (RAG) chain.
    """
    
    # Create a vector store from the Oracle connection and embeddings
    vector_store = db_doc_service.get_vector_store()

    retriever = vector_store.as_retriever(
        search_kwargs={
            "k": 100
        }
    )

    # Define a prompt template for retrieval-augmented generation (RAG) using the retrieved context fragments
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", (
                """
                You are an assistant for question-answering tasks.
                Please use only the following retrieved context fragments to answer the question.
                If you don't know the answer, say you don't know.
                Always use all available data.

                {context}
                """
            )),
            ("human", "{input}")
        ]
    )

    # Create a chain that uses the LLM to combine the retrieved documents into a final answer
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    
    # Build the full retrieval chain that first retrieves documents and then uses them to answer the input query
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    # Invoke the retrieval chain with the given input query to generate a response
    response = rag_chain.invoke({"input": input})

    # Extract the generated answer from the chain's response
    content = response['answer']
    
    return content